{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random\n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "It's an algorithm that tries to find the minimum of a function (here, the cost function) by looking at the gradient and moving downhill towards the nearest low point in sight. Without any optimization, there are three types of gradient descent. \n",
    "\n",
    "### Batch Gradient Descent / vanilla gradient descent\n",
    "\n",
    "Here, the cost function is calculated over all data points for ONE update. Hence it's slow, both by time and space. \n",
    "Below is a simple implementation of BGD. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, Y, alpha, numIterations):\n",
    "\n",
    "    ''' Input : \n",
    "        x : M*N sized ndarray \n",
    "        y : Array of length m, contains target values\n",
    "        alpha : learning rate ( the coefficient applied to the gradient )\n",
    "        numIterations : Max no. of iterations of correcting the coefficients \n",
    "        ''' \n",
    "    \n",
    "    m = len(X)\n",
    "    X1 = np.ones(shape=(len(X),2))\n",
    "    theta = np.random.rand(2)\n",
    "    cost = np.zeros(numIterations)\n",
    "    X1[:,1] = X\n",
    "    xTrans = X1.T\n",
    "    x_range = (min(X1[:,1]),max(X1[:,1]))\n",
    "    x_axis = np.linspace(x_range[0],x_range[1], num = 200)\n",
    "    \n",
    "    \n",
    "    for i in range(numIterations):\n",
    "         \n",
    "        hypothesis = np.dot(X1,theta)\n",
    "        loss = hypothesis - Y\n",
    "        mse_cost = np.sum(np.square(loss))/(m)\n",
    "        cost[i] = mse_cost\n",
    "        print(\"Iteration %d | Cost: %f\" % (i, mse_cost))\n",
    "        gradient = - 2*np.dot(xTrans,loss)/m\n",
    "        theta = theta + alpha*gradient\n",
    "        plot_lines(theta,X,Y)    \n",
    "    \n",
    "    plt.plot(cost)\n",
    "    print(theta, mse_cost)    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to plot lines as they fit the data\n",
    "\n",
    "\n",
    "\n",
    "def plot_lines(coeffs,X,Y):\n",
    "    x_range = (min(X),max(X))\n",
    "    x_axis = np.linspace(x_range[0],x_range[1], num = 20)\n",
    "    y = coeffs[0] + coeffs[1]*x_axis\n",
    "    \n",
    "    plt.scatter(X,Y)\n",
    "    plt.plot(x_axis,y,color='r',linewidth = 0.5)\n",
    "    #plt.plot(cost)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('data/insurance_data.csv',delimiter=',')\n",
    "X = data[:,0] \n",
    "Y = data[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientDescent(X,Y,0.0001,30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = np.genfromtxt('data/curve80.txt',delimiter='')\n",
    "X2 = data2[:,0]\n",
    "Y2 = data2[:,1]\n",
    "#print(Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradientDescent(X2,Y2,0.01,900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "In this algorithm, we calculate the cost function for every data point, constantly updating the coefficients as we\n",
    "face new data points. This is an online algorithm as it can adapt to new values. After every interation, we reshuffle the data to avoid any bias due to ordering of the data. Below is a function that calculates Stchastic Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StochasticGD(data,alpha,numIterations):\n",
    "\n",
    "    ''' Input : \n",
    "        x : M*N sized ndarray \n",
    "        y : Array of length m, contains target values\n",
    "        alpha : learning rate ( the coefficient applied to the gradient )\n",
    "        numIterations : Max no. of iterations of correcting the coefficients \n",
    "        ''' \n",
    "    x = data[:,0]\n",
    "    y = data[:,1]\n",
    "    theta = np.random.rand(2)\n",
    "    m = len(x)\n",
    "    cost = np.zeros(numIterations)\n",
    "    x_range = (min(x),max(x))\n",
    "    x_axis = np.linspace(x_range[0],x_range[1], num = 20)\n",
    "    \n",
    "    for i in range(numIterations):\n",
    "        \n",
    "        np.random.shuffle(data)\n",
    "        X = data[:,0]\n",
    "        X1 = np.ones(shape=(m,2))\n",
    "        X1[:,1] = X\n",
    "        xTrans = X1.T\n",
    "        Y = data[:,1]\n",
    "        \n",
    "        hypothesis = np.dot(X1,theta)   #update the hypothesis from the previous theta\n",
    "        loss = hypothesis - Y\n",
    "        mse_cost = sum(np.square(loss))/m\n",
    "        print(\"Iteration %d | Cost: %f\" % (i, mse_cost))\n",
    "        \n",
    "        for data_point in range(m):\n",
    "            \n",
    "            gradient = - 2*np.dot(loss[data_point],X1[data_point])\n",
    "            theta = theta + alpha*gradient\n",
    "            \n",
    "        \n",
    "        plot_lines(theta,x,y)  \n",
    "        cost[i] = mse_cost \n",
    "    \n",
    "    \n",
    "    plt.plot(cost)\n",
    "    plt.show()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StochasticGD(data,0.00001,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StochasticGD(data2,0.0002,600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard and Soft SGD\n",
    "\n",
    "\n",
    "In the above function, we calculate the loss for each epoch and then further update the values of theta in each iteration\n",
    "but in one epoch, we are NOT updating the value of the hypothesis/loss. So, this can be called a 'soft' Stochastic \n",
    "Gradient Descent. \n",
    "\n",
    "In the below example, however, we update the value of hypothesis and hence, loss after each iteration. \n",
    "So the hypothesis and loss gets updated in every iteration of every epoch. This can be called 'hard' SGD. \n",
    "\n",
    "In the later algortihm, the plot of the cost function seems dense because more number of mean square errors are \n",
    "collected. (m*numIterations number of L2 errors are collected in contrast to m errors in Batch Gradient Descent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(data,alpha,numIterations):\n",
    "\n",
    "    ''' Input : \n",
    "        x : M*N sized ndarray \n",
    "        y : Array of length m, contains target values\n",
    "        alpha : learning rate ( the coefficient applied to the gradient )\n",
    "        numIterations : Max no. of iterations of correcting the coefficients \n",
    "        ''' \n",
    "    x = data[:,0]\n",
    "    y = data[:,1]\n",
    "    theta = np.random.rand(2)\n",
    "    m = len(x)\n",
    "    #cost = np.zeros(numIterations*m)\n",
    "    cost = [] \n",
    "    x_range = (min(x),max(x))\n",
    "    x_axis = np.linspace(x_range[0],x_range[1], num = 20)\n",
    "    \n",
    "    for i in range(numIterations):\n",
    "        \n",
    "        np.random.shuffle(data)\n",
    "        X = data[:,0]\n",
    "        X1 = np.ones(shape=(m,2))\n",
    "        X1[:,1] = X\n",
    "        xTrans = X1.T\n",
    "        Y = data[:,1]\n",
    "        #hypothesis = np.dot(X1,theta)\n",
    "        #loss = hypothesis - Y\n",
    "        \n",
    "        \n",
    "        \n",
    "        for data_point in range(m):\n",
    "            \n",
    "            hypothesis = np.dot(X1,theta)\n",
    "            loss = hypothesis - Y\n",
    "            gradient = - 2*np.dot(loss[data_point],X1[data_point])\n",
    "            theta = theta + alpha*gradient\n",
    "            mse_cost = sum(np.square(loss))/m\n",
    "            cost.append(mse_cost) \n",
    "        \n",
    "        \n",
    "        plot_lines(theta,x,y)  \n",
    "#         mse_cost = sum(np.square(loss))/m\n",
    "#         cost[i] = mse_cost \n",
    "        print(\"Iteration %d | Cost: %f\" % (i, mse_cost))\n",
    "    \n",
    "    plt.plot(cost)\n",
    "    plt.show()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD(data2,0.0003,600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD(data,0.00002,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some notes : \n",
    "How do you differentiate between Hard and Soft SGD ? \n",
    "In hard SGD, the values are updated more frequently so the curve has significant fluctuations. The fluctuation also increases the probability of jumping to a new and better minima. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
